#!/usr/bin/env python3
"""
LightRAG Initialization for Obsidian RAG Stack
Compatible with LightRAG v1.4.9.4 and Ollama local backend
"""

import os
import asyncio
from pathlib import Path
from lightrag import LightRAG, QueryParam
from lightrag.llm.ollama import ollama_model_complete, ollama_embed
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import EmbeddingFunc, setup_logger


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

WORKING_DIR = "./lightrag_db"
VAULT_DIR = "./vault"  # Mounted host folder: ~/Documents/ObsidianVault â†’ /app/vault

LLM_MODEL_NAME = "qwen2.5:7b"
EMBED_MODEL_NAME = "nomic-embed-text"

# Optionally expand context window for Ollama
LLM_MODEL_KWARGS = {"options": {"num_ctx": 32768}}

# Setup logging
setup_logger("lightrag", level="INFO")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Load Markdown Notes from Obsidian Vault
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_vault_texts(vault_path: str):
    """Recursively load all Markdown files from the vault folder"""
    notes = []
    for md_file in Path(vault_path).rglob("*.md"):
        try:
            with open(md_file, "r", encoding="utf-8") as f:
                content = f.read().strip()
                if content:
                    notes.append(content)
        except Exception as e:
            print(f"âš ï¸ Could not read {md_file}: {e}")
    print(f"âœ… Loaded {len(notes)} Markdown notes from {vault_path}")
    return notes


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LightRAG Initialization
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ["OLLAMA_HOST"] = "http://host.docker.internal:11434"

async def initialize_rag():
    """Create and initialize LightRAG with Ollama LLM and embedding"""
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=ollama_model_complete,
        llm_model_name=LLM_MODEL_NAME,
        llm_model_kwargs=LLM_MODEL_KWARGS,
        embedding_func=EmbeddingFunc(
            embedding_dim=768,
            func=lambda texts: ollama_embed(texts, embed_model=EMBED_MODEL_NAME)
        ),
    )
    await rag.initialize_storages()
    await initialize_pipeline_status()
    return rag


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main async Entry Point
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main():
    rag = await initialize_rag()

    # Step 1: Load Notes
    notes = load_vault_texts(VAULT_DIR)

    # Step 2: Insert Notes
    if notes:
        print("ğŸ“¥ Inserting notes into LightRAG...")
        await rag.ainsert(notes)
        print("âœ… Notes successfully indexed.")

    # Step 3: Run a Sample Query
    param = QueryParam(mode="hybrid")
    query = "How does my treatment relate to outcomes?"
    print(f"\nğŸ” Querying: {query}\n")
    result = await rag.aquery(query, param=param)
    print("ğŸ’¬ Query Result:")
    print(result)

    await rag.finalize_storages()
    print("\nâœ… LightRAG session complete.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Entrypoint
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    asyncio.run(main())
